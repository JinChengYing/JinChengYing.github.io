---
layout: post
title: "误差与风险"
date:   2025-5-22
tags: [Machine learning,learning theory]
comments: true
author: Jincheng Ying

---

###  bias与variance  vs 误差

首先需要明确, bias(偏差)和variance 这对概念与风险导出的误差是有本质的不同的,偏差与方差是衡量某一个模型的预测效果, 风险导出的近似误差(approximation error)与估计误差(estimation error)是用来衡量model class,即一类函数族(神经网络等),也称为假设空间,的表达能力与泛化性能.



#### 从泛化误差到偏差与方差

假设采样数据集为$D=\{(x_i,y_i)\}_{i=1,2,\dots,n}$, 目标是用$\hat{f}$估计真实值$y$,$f$是在数据集上学到的predictor.

$y_D$通过函数$f$加上噪声数据生成,$y=f(x)+\epsilon$,一个常用的假设是噪声均值为$0$

假设损失函数是均方误差$l(x,y)=(y-\hat{f}(x))^2$

那么对于泛化误差的期望(数据集D的内在分布求loss的期望)
$$
E(f;D)  =E_D[(y_D- \hat{f}(x))^2]\\
     =E_D[(y_D-E_D[\hat{f}(x)]+E_D[\hat{f}(x)]-\hat{f}(x))]
\\
  = E_D[(y_D-E_D[\hat{f}(x)])^2]+E_D[(E_D[\hat{f}(x)]-\hat{f}(x))^2]
  \\
  =E_D[(y_D-y+y-E_D[\hat{f}(x)])^2]+E_D[(E_D[\hat{f}(x)]-\hat{f}(x))^2]
  \\
  =E_D[(\hat{f}(x)-E_D[\hat{f}(x)])^2]+(E_D[\hat{f}(x)]-y)^2+E_D[y_D-y]^2
\\  =var(x)+bias^2(x)+\epsilon^2
$$

- 上面结果的第一项,是预测的期望值到预测值的距离,是方差var(x),度量了预测结果的稳定性,表示了数据集变化对学习性能的影响
- 第二项衡量了预测的期望值到真实值的距离,是偏差bias(x)的平方,刻画了算法的拟合能力

![image-20250524194722380](https://JinChengYing.github.io/images/image-20250524194722380.png)

将预测理解为射靶子,偏差的含义是能否射中靶心,方差的含义是射中的目标是否接近(即结果是否稳定).

![image-20250524200536171](https://JinChengYing.github.io/images/image-20250524200536171.png)

用这张图一样可以理解,在假设空间(预测器所在的函数集合)里,泛化误差是实际的$f(x)$到预测的$f'$的距离,可以拆解为预测值到预测值的期望的距离(即方差)加上预测值的期望到最优值的的差距(即偏差,即预测器预测的准确性,是否射中了靶子)

**偏差与方差的权衡**

当训练程度较低时,偏差较大,欠拟合,此时偏差较大;

随着训练程度增大,此时偏差减小,但是数据集的轻微变化会显著影响学习器,容易发生过拟合,方差不断增大.(越来越体现训练数据集的特性,离底层的分布越来越远)



#### 风险

风险是对模型类(model class)的表达能力和泛化能力的评估

假设损失函数为$l(y,z)$,$z$是标签$y$的预测值

**expected risk期望风险$\mathcal{R}(f)$(也称作泛化误差或testing error)**

![image-20250526224457355](https://JinChengYing.github.io/images/image-20250526224457355.png)

期望风险的随机性来源于数据集的分布$p$,即依赖于训练数据集,因此其为预测函数$f$的函数

**empirical risk经验风险$\hat{\mathcal{R}}(f)$(也称作训练误差)**

![image-20250526224640578](https://JinChengYing.github.io/images/image-20250526224640578.png)

实际上是对训练集上的损失取均值.

**bayes risk$\mathcal{R}^*$**(期望风险的最小化的期望)

![image-20250524211436324](https://JinChengYing.github.io/images/image-20250524211436324.png)

期望风险的最小化不总是唯一,但是bayes risk唯一,在有监督学习问题中,bayes risk是理想最优的表现

**Excess risk额外风险**

![image-20250524211654240](https://JinChengYing.github.io/images/image-20250524211654240.png)

#### 风险分解到估计误差,近似误差

对于参数空间中的任意$\hat{\theta}$,其对应的预测器的额外风险为:

![image-20250524211906517](https://JinChengYing.github.io/images/image-20250524211906517.png)

其中第一项的estimation error 为估计误差(**统计性误差**),即为假设空间内,当前model到假设空间内最优model(期望风险最小化)的距离,衡量的是当前算法估计的表达能力,并且可以进一步展开

![image-20250524212401201](https://JinChengYing.github.io/images/image-20250524212401201.png)

其中的经验优化误差衡量的是优化过程中,给出的预测器到经验风险最小化的距离![image-20250524214706928](https://JinChengYing.github.io/images/image-20250524214706928.png)

![image-20250524214522506](https://JinChengYing.github.io/images/image-20250524214522506.png)

第二项的approximation error为近似误差,即为假设空间内最优的model(期望风险最小化)到目标空间最优model的距离,衡量了假设空间的表达能力.

上述的分解是针对经验风险最小化的excess risk,对于某一算法的额外风险,其分解为:![image-20250524214946510](https://JinChengYing.github.io/images/image-20250524214946510.png)

 



他们的关系可以用这张图表示

![image-20250524214015947](https://JinChengYing.github.io/images/image-20250524214015947.png)

 在算法迭代过程中产生的优化误差,可以通过算法改进而改进;近似误差受到假设空间的影响,通过分析上的手段改进,比如假设预测器所在函数空间(holder RKHS等);估计误差受到数据集的影响,主要在统计上进行改进.

**模型选择的影响**

当假设空间扩大,即参数空间变大,模型的表达能力增强,近似误差减小到0,与样本量无关

估计误差受到学习到的model的影响,样本量越大,估计越准确,估计误差越小;假设空间增大时,收敛会更慢,估计误差会增加











